\title{Failz -- Failing Fast}
\author{
        ... \\
        ... \\
            \and
        ... \\
        ... \\
}
%\date{\today}

\documentclass[12pt]{article}

\usepackage{amsmath}

\begin{document}
\maketitle

%\begin{abstract}
%This is the paper's abstract \ldots
%\end{abstract}

\section{Introduction}
Given a test suite for a software project, we may want to detect failures in the suit early. For example, we may cancel the entire test run on a single failure (and ask the submitter to fix it before resubmitting), or, even if we do let the tests run to completion, we may want to see errors early so we can start to work on them while the tests are still running.

Some tests may be `canaries` that are more likely to fail if something is wrong: for example, a test that uses multiple features may fail if any of those features was just broken, while unit tests will only fail depending on the specific feature they care about. We would like to run the more sensitive, likely-to-fail tests first. More specifically, if $f(i)$ is the event of the $i$th test failing, $t(i)$ is the time it takes to run the  $i$th test, and $s : \{ 1\cdots n \} \rightarrow_{1:1} \{ 1 \cdots n \}$ is a permutation, then the expected time til the first failure is
\[ L(s) \; = \; \sum_{i=1}^{n} P\Big(\neg f(s(1)),\cdots,\neg f(s(i-1)) \;,\;f(s(i)) \Big) \cdot \sum_{j=1}^i t(s(j))  \]
since if tests passed up to time $i$, then failed, then the total time is the sum of the times until that point. (Note that we might want to scale by the probability of at least 1 test failing, but it does not affect the results; i.e., as written, the loss sort of assumes at least one test will fail.)

\subsection{Naive Approaches}

For simplicity, we can ignore test times for now and assume they all have the same time $t = 1$. One simple approach is then to iterate and greedily pick the test which is most likely to fail at each point in time. That is, we start by picking
\[ s(1) = \textbf{argmax}_{i} P(f(i)) \]
We then continue to pick the next test based on which is most likely to fail, assuming all the previous have \emph{not} failed,
\[ s(k+1) = \textbf{argmax}_{i} P\Big(f(i) \; \Big| \; \neg f(s(1)), \cdots , \neg f(s(k)) \Big) \]

This seems like a reasonable approach, since at each point in time we pick the test that will maximize our immediate benefit. However, it is not guaranteed to reach the optimal result. For example, assume $x$ and $y$ are independent random variables, each failing with probability $\frac{1}{2}$. Assume that $z$ is a random variable that with probability $\frac{2}{3} + \epsilon$ (for a small $\epsilon$) will fail if at least one of $x$ and $y$ fails, and otherwise simply passes (as a concrete example of how this can occur, $z$ may use a subset of features tested in both $x$ and $y$). Then $z$ fails with probability $\frac{1}{2} + \delta$ for some small $\delta>0$, causing the greedy algorithm to pick it first. But then, no matter which of the other two variables we pick next, we are doing worse than if we picked $x$ and $y$ first, since after picking $x$ and $y$, there is no chance of $z$ failing, or, more specifically, if we pick $x$ and $y$ first, the loss is $\frac{1}{2} + 2 \cdot \frac{1}{4} = 1$, but if we pick $z$ first, we have
\[ \frac{1}{2} + \delta + 2 \cdot \frac{1}{2} \cdot \Bigg( \frac{1}{3} - \epsilon \Bigg) + 3 \cdot \frac{1}{2} \cdot \Bigg( \frac{1}{3} - \epsilon \Bigg) \cdot \frac{1}{2}
\; = \;
\frac{13}{12} + \gamma(\epsilon, \delta)\]
where the last expression $\gamma$ depends only on $\epsilon$ and $\delta$, and which we can take as close to $0$ as we want by reducing $\epsilon$, thus making the result larger than $1$ which as we saw before is the optimal result.

The problem is that we too eagerly choose $z$ in this example: It is indeed more likely to fail, but offers no really useful information in the end. Motivated by this, we can consider the "opposite" algorithm, of picking the "apparently useless" indexes at the end, that is, we start with
\[ s(n) = \textbf{argmin}_{i} P\Big(f(i) \; \Big| \; \forall j \neq i \;\; \neg f(j) \Big) \]
While this helps with the above example, it is not guaranteed to suceed either, and can fail far more horribly in fact. Assume we have $n$ tests that fail with probability $1$, and a last test that fails if any of the others do. The last test should obviously be attempted first, as for large $n$ it will almost certainly fail, however, the second naive algorithm with put it last.

What can we do?

Binary ICA might find the underlying "core unit tests" that the others depend on. Still, this doesn't tell us which to put first.

\section{Hill Climbing via Bubble-Sorting}

We can generate a large amount of random permutatons, and improve them by hill-climbing, by means of flipping two adjacent tests, so for example from $1,2,3,4,5$ to $2,1,3,4,5$. A flip alters the loss thus only on two elements of the sum (since, after them the conditional probability remains the same), so if we flip $k$ and $k+1$, generating a new permutation $\hat{s}$, the difference is:
\begin{align}
  &\sum_{i=0}^{i=k} t(\hat{s}(i)) \cdot P\Big(f(\hat{s}(k)) \; \Big| \; \neg f(\hat{s}(0)), \cdots, \neg f(\hat{s}(k-1)) \Big) &+ \\
  &\sum_{i=0}^{i=k+1} t(\hat{s}(i)) \cdot P\Big(f(\hat{s}(k+1)) \; \Big| \; \neg f(\hat{s}(0)), \cdots, \neg f(\hat{s}(k)) \Big) &- \\
  &\sum_{i=0}^{i=k} t(s(i)) \cdot P\Big(f(s(k)) \; \Big| \; \neg f(s(0)), \cdots, \neg f(s(k-1)) \Big) &- \\
  &\sum_{i=0}^{i=k+1} t(s(i)) \cdot P\Big(f(s(k+1)) \; \Big| \; \neg f(s(0)), \cdots, \neg f(s(k)) \Big) = \\
  &\Big[ t(s(i+1)) - t(s(i)) + \sum_{i=0}^{i=k} t(s(i)) \Big] \cdot P\Big(f(s(k+1)) \; \Big| \; \neg f(s(0)), \cdots, \neg f(s(k-1)) \Big) &+ \\
  &\sum_{i=0}^{i=k+1} t(s(i)) \cdot P\Big(f(s(k)) \; \Big| \; \neg f(s(0)), \cdots, \neg f(s(k-1)), \neg f(s(k+1)) \Big) &- \\
  &\sum_{i=0}^{i=k} t(s(i)) \cdot P\Big(f(s(k)) \; \Big| \; \neg f(s(0)), \cdots, \neg f(s(k-1)) \Big) &- \\
  &\sum_{i=0}^{i=k+1} t(s(i)) \cdot P\Big(f(s(k+1)) \; \Big| \; \neg f(s(0)), \cdots, \neg f(s(k)) \Big)
\end{align}

Are there local minima under this approach?

\section{Algorithm X}

Sample over all possible orderings, and correlate the time to first fail with the location of each test. An test whose low position is positively correlated with an early first fail is a promising test to put early up. After picking the first element in this way, we continue iteratively, under the assumption the first test did not fail (i.e. we drop random samples were it failed).

Later iterations can in fact use a distribution that slowly hardens, based on what we already know is likely good. E.g. fix the first, then recurse on the rest, etc. But does not need to be the first?

\section{Fuzzing a Test Suite}

Introduce random noise in the project, e.g. by replacing a number with a modified number (will still compile!), to get raw data.

Perhaps interesting to see how many fuzz-inducted test failures are caught at compile time or via a runtime assert. Sign of a robust project. (Of course, we are not seeing breakage that is caught by neither tests, compile time or runtime asserts; this is mainly useful for a project with a large an effective test suite.)


%\bibliographystyle{abbrv}
%\bibliography{main}

\end{document}

